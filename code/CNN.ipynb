{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from tflearn.data_utils import shuffle\n",
    "\n",
    "class CNN(object):\n",
    "    def __init__(self, patch_size, num_filters_fist_layer, num_filters_second_layer,\n",
    "                 size_fully_connected_layer, image_x=400, image_y=400, image_channels=4, num_classes=10):\n",
    "        \n",
    "        self.X_train = None\n",
    "        self.Y_train = None\n",
    "        self.X_test = None\n",
    "        self.Y_test = None\n",
    "        self.image_x = image_x\n",
    "        self.image_y = image_y\n",
    "        self.image_channels = image_channels\n",
    "        image_size = self.image_x * self.image_y\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, image_x, image_y, image_channels])\n",
    "        self.y_ = tf.placeholder(tf.float32, shape=[None, num_classes])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        def weight_variable(shape, nameVar):\n",
    "            initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "            return tf.Variable(initial, name=nameVar)\n",
    "\n",
    "        def bias_variable(shape, nameVar):\n",
    "            initial = tf.constant(0.1, shape=shape)\n",
    "            return tf.Variable(initial, name=nameVar)\n",
    "\n",
    "        def conv2d(x, W):\n",
    "            return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "        def max_pool_2x2(x):\n",
    "            return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "\n",
    "        # First Layer (Convolution and Max Pool)\n",
    "        self.W_conv1 = weight_variable([patch_size, patch_size, image_channels, num_filters_fist_layer], \"filter_layer1\")\n",
    "        b_conv1 = bias_variable([num_filters_fist_layer], \"bias_layer1\")\n",
    "        x_image = tf.reshape(self.x, [-1, image_x, image_y, image_channels])\n",
    "        # Apply Convolution and Max Pool\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, self.W_conv1) + b_conv1)\n",
    "        print(h_conv1.get_shape())\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "        print(h_pool1.get_shape())\n",
    "\n",
    "        # Second Layer (Convolution and Max Pool)\n",
    "        self.W_conv2 = weight_variable([patch_size, patch_size, num_filters_fist_layer, num_filters_second_layer], \"filter_layer2\")\n",
    "        b_conv2 = bias_variable([num_filters_second_layer], \"bias_layer2\")\n",
    "        # Apply Convolution and Max Pool\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, self.W_conv2) + b_conv2)\n",
    "        print(h_conv2.get_shape())\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "        print(h_pool2.get_shape())\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        W_fc1 = weight_variable([int(image_x / 4) * int(image_y / 4) * num_filters_second_layer, size_fully_connected_layer], \"W_fc1\")\n",
    "        b_fc1 = bias_variable([size_fully_connected_layer], \"b_fc1\")\n",
    "\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, int(image_x / 4) * int(image_y / 4) * num_filters_second_layer])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "        print(h_fc1.get_shape())# the shape of h_fc1 is [-1, size_fully_connected_layer]\n",
    "\n",
    "        # Add dropout\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, self.keep_prob)\n",
    "\n",
    "        # Add the last fully connected layer for output\n",
    "        W_fc2 = weight_variable([size_fully_connected_layer, num_classes], \"W_fc2\")\n",
    "        b_fc2 = bias_variable([num_classes], \"b_fc2\")\n",
    "        l2_loss = 0.0\n",
    "        l2_loss += tf.nn.l2_loss(W_fc2)\n",
    "        l2_loss += tf.nn.l2_loss(b_fc2)\n",
    "\n",
    "        self.y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "        self.cross_entropy = tf.reduce_mean(-tf.reduce_sum(self.y_ * tf.log(self.y), reduction_indices=[1]))\n",
    "        \n",
    "#        self.y = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "#        self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_, logits=self.y))\n",
    "\n",
    "#        self.train = tf.train.AdamOptimizer(1e-4).minimize(self.cross_entropy)\n",
    "        self.train = tf.train.GradientDescentOptimizer(1e-4).minimize(self.cross_entropy)\n",
    "        \n",
    "        self.correct_prediction = tf.cast(tf.equal(tf.argmax(self.y, 1), tf.argmax(self.y_, 1)), tf.float32) + 1e-6 * l2_loss\n",
    "        self.accuracy = tf.reduce_mean(self.correct_prediction)\n",
    "        \n",
    "    def load_data(self, dir='/home/anshul/powerSpectrograms200/'):\n",
    "        self.X_train = np.zeros((800, self.image_x, self.image_y, self.image_channels))\n",
    "        self.Y_train = np.zeros((800,), dtype=int)\n",
    "        self.X_test = np.zeros((200, self.image_x, self.image_y, self.image_channels))\n",
    "        self.Y_test = np.zeros((200,), dtype=int)\n",
    "\n",
    "        genres = {'blues':     0,\n",
    "                  'classical': 1,\n",
    "                  'country':   2,\n",
    "                  'disco':     3,\n",
    "                  'hiphop':    4,\n",
    "                  'jazz':      5,\n",
    "                  'metal':     6,\n",
    "                  'pop':       7,\n",
    "                  'reggae':    8,\n",
    "                  'rock':      9}\n",
    "\n",
    "        indexTrain = 0\n",
    "        indexTest = 0\n",
    "        for genre in genres.keys():\n",
    "            for count in range(0, 100):\n",
    "                path = dir + genre + '/' + genre + '.%0.5d' % count + '.au.png'\n",
    "                if os.path.isfile(path):\n",
    "                    if count < 80:\n",
    "                        self.X_train[indexTrain] = plt.imread(path)\n",
    "                        self.Y_train[indexTrain] = genres[genre]\n",
    "                        indexTrain += 1\n",
    "                    else:\n",
    "                        self.X_test[indexTest] = plt.imread(path)\n",
    "                        self.Y_test[indexTest] = genres[genre]\n",
    "                        indexTest += 1\n",
    "                        \n",
    "        Y_train_onehot = np.zeros((self.Y_train.shape[0], self.num_classes))\n",
    "        Y_train_onehot[np.arange(self.Y_train.shape[0]), self.Y_train] = 1\n",
    "        \n",
    "        Y_test_onehot = np.zeros((self.Y_test.shape[0], self.num_classes))\n",
    "        Y_test_onehot[np.arange(self.Y_test.shape[0]), self.Y_test] = 1\n",
    "        \n",
    "        self.Y_train = Y_train_onehot\n",
    "        self.Y_test = Y_test_onehot\n",
    "        \n",
    "        self.X_train, self.Y_train = shuffle(self.X_train, self.Y_train)\n",
    "                        \n",
    "\n",
    "cnn = CNN(image_x=200,\n",
    "          image_y=200,\n",
    "          image_channels=4,\n",
    "          num_classes=10,\n",
    "          num_filters_fist_layer=80,\n",
    "          num_filters_second_layer=80,\n",
    "          patch_size=5,\n",
    "          size_fully_connected_layer=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "\n",
    "    train_batches = 16\n",
    "    test_batches = 10\n",
    "\n",
    "    n_train_images = 800\n",
    "    n_test_images = 200\n",
    "\n",
    "    train_step_size = int(n_train_images / train_batches)\n",
    "    test_step_size = int(n_test_images / test_batches)\n",
    "\n",
    "    for i in range(0, n_train_images, train_step_size):\n",
    "        print(\"Batch\", i, \"to\", i + train_step_size - 1)\n",
    "        X_train = cnn.X_train[i : i + train_step_size]\n",
    "        Y_train = cnn.Y_train[i : i + train_step_size]\n",
    "        \n",
    "        feed_dict = {cnn.x : X_train, cnn.y_ : Y_train, cnn.keep_prob : 1.0}\n",
    "        session.run(cnn.train, feed_dict)\n",
    "        \n",
    "        trainAccuracy = session.run(cnn.accuracy, feed_dict)\n",
    "        print(\"Train Accuracy:\", trainAccuracy)\n",
    "        \n",
    "        feed_dict = {cnn.x : cnn.X_test, cnn.y_ : cnn.Y_test, cnn.keep_prob : 1.0}\n",
    "        testAccuracy = session.run(cnn.accuracy, feed_dict)\n",
    "        print(\"Test Accuracy: \", testAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
